{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-04-08 23:27:56.110756: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import tensorflow as tf\n",
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np\n",
    "from sklearn import preprocessing\n",
    "import scipy.io\n",
    "from tensorflow.keras import layers\n",
    "import mne\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "from tensorflow.keras.models import load_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31mFailed to connect to the remote Jupyter Server 'http://192.168.50.50:8888/'. Verify the server is running and reachable. (Denied connection to insecure server.)."
     ]
    }
   ],
   "source": [
    "#load data from ../EEG folder, all csv files\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'\n",
    "tf.get_logger().setLevel('ERROR')\n",
    "\n",
    "def get_data():\n",
    "    data = []\n",
    "    scores = []\n",
    "    order = []\n",
    "    filedir = '/mnt/Ryans Study/EEG'\n",
    "    for file in [f for f in os.listdir(filedir) if f.endswith(\".fif\") and not f.endswith(\"resting.fif\") and not f.endswith(\"hard.fif\")]:\n",
    "        filepath = filedir + \"/\" + file\n",
    "        print(file)\n",
    "        # get number from file name\n",
    "        order.append(int(file.split(\"_\")[0]))\n",
    "        # load raw file\n",
    "        raw = mne.io.read_raw_fif(filepath, preload=True)\n",
    "        # get data\n",
    "        data.append(raw.get_data(picks=['Fp1','Fp2'])[:, 7500:-15000])\n",
    "        # get scores from file names, 1 = watching, 2 = normal, 3 = hard\n",
    "        if \"watching\" in file or \"watch\" in file:\n",
    "            scores.append(0)\n",
    "        elif \"normal\" in file or \"correct\" in file:\n",
    "            scores.append(1)\n",
    "        elif \"hard\" in file:\n",
    "            scores.append(2)\n",
    "\n",
    "    return data, scores, order\n",
    "\n",
    "\n",
    "def remove_participant(data, scores, order, participant):\n",
    "    newdata = []\n",
    "    newscores = []\n",
    "    removed_participant = []\n",
    "    removed_scores = []\n",
    "\n",
    "    for x, y, z in zip(data, scores, order):\n",
    "        if z != participant:\n",
    "            newdata.append(x)\n",
    "            newscores.append(y)\n",
    "        else:\n",
    "            removed_participant.append(x)\n",
    "            removed_scores.append(y)\n",
    "\n",
    "    return np.array(newdata), np.array(newscores), np.array(removed_participant), np.array(removed_scores)\n",
    "\n",
    "\n",
    "def split_timeseries(series, window_size=1000, overlap=100):\n",
    "    segments = []\n",
    "    for i in range(0, series.shape[-1] - window_size + 1, window_size - overlap):\n",
    "        segment = series[..., i:i + window_size]\n",
    "        # add extra dimension for channel\n",
    "        x_max = np.max(segment)\n",
    "        x_avg = np.mean(segment)\n",
    "        segment = (segment - x_avg) / x_max\n",
    "        segment = np.expand_dims(segment, axis=-1)\n",
    "        segments.append(segment)\n",
    "    return segments\n",
    "\n",
    "\n",
    "# split data into 1000 sample sliding window with 100 sample overlap\n",
    "def split_data(data, scores, order, window_size=1000, overlap=100):\n",
    "    X = []\n",
    "    Y = []\n",
    "    neworder = []\n",
    "    for x, y, z in zip(data, scores, order):\n",
    "        x = split_timeseries(x, window_size, overlap)\n",
    "        X.extend(x)\n",
    "        Y.extend([y] * len(x))\n",
    "        neworder.extend([z] * len(x))\n",
    "    return X, Y, neworder\n",
    "\n",
    "\n",
    "def remove_nan(data, scores, order):\n",
    "    # if series contains nan, remove it\n",
    "    newdata = []\n",
    "    newscores = []\n",
    "    neworder = []\n",
    "    for x, y, z in zip(data, scores, order):\n",
    "        if np.isnan(x).any():\n",
    "            continue\n",
    "        else:\n",
    "            newdata.append(x)\n",
    "            newscores.append(y)\n",
    "            neworder.append(z)\n",
    "    return newdata, newscores, neworder\n",
    "\n",
    "\n",
    "# def model\n",
    "def EEGNet_seq(nb_classes, Chans=64, Samples=128,\n",
    "               dropoutRate=0.5, kernLength=64, F1=8,\n",
    "               D=2, F2=16, norm_rate=0.25, dropoutType='Dropout',\n",
    "               learning_rate=3e-3, loss=\"mse\"):\n",
    "    \"\"\"Create a Sequential EEGNet model.\n",
    "\n",
    "      nb_classes      : int, number of classes to classify\n",
    "      Chans, Samples  : number of channels and time points in the EEG data\n",
    "      dropoutRate     : dropout fraction\n",
    "      kernLength      : length of temporal convolution in first layer. We found\n",
    "                        that setting this to be half the sampling rate worked\n",
    "                        well in practice. For the SMR dataset in particular\n",
    "                        since the data was high-passed at 4Hz we used a kernel\n",
    "                        length of 32.\n",
    "      F1, F2          : number of temporal filters (F1) and number of pointwise\n",
    "                        filters (F2) to learn. Default: F1 = 8, F2 = F1 * D.\n",
    "      D               : number of spatial filters to learn within each temporal\n",
    "                        convolution. Default: D = 2\n",
    "      dropoutType     : Either SpatialDropout2D or Dropout, passed as a string.\n",
    "\n",
    "    :param nb_classes: int, number of classes to classify\n",
    "    :param Chans: Number of channels in the EEG data\n",
    "    :param Samples: Number of time poitns in EEG data\n",
    "    :param dropoutRate: dropout fraction\n",
    "    :param kernLength: Length of temporal convolution in first layer\n",
    "    :param F1: Number of temporal features to learn\n",
    "    :param D: Number of spatial filters to learn within each temporal\n",
    "    convolution\n",
    "    :param F2: Number of pointwise filters to learn\n",
    "    :param norm_rate: Normalisation rate\n",
    "    :param dropoutType: Dropout method to use\n",
    "    :param learning_rate: Learning rate of classifier\n",
    "    :param loss: Loss function of classifier\n",
    "    :returns: Keras Sequential model\n",
    "\n",
    "    \"\"\"\n",
    "    from tensorflow.keras.models import Model\n",
    "    from tensorflow.keras.models import Sequential\n",
    "    from tensorflow.keras.layers import Dense, Activation, Dropout\n",
    "    from tensorflow.keras.layers import Conv2D, AveragePooling2D\n",
    "    from tensorflow.keras.layers import SeparableConv2D, DepthwiseConv2D\n",
    "    from tensorflow.keras.layers import BatchNormalization\n",
    "    from tensorflow.keras.layers import SpatialDropout2D\n",
    "    from tensorflow.keras.layers import Input, Flatten\n",
    "    from tensorflow.keras.constraints import max_norm\n",
    "    from tensorflow import keras\n",
    "\n",
    "    if dropoutType == 'SpatialDropout2D':\n",
    "        dropoutType = SpatialDropout2D\n",
    "    elif dropoutType == 'Dropout':\n",
    "        dropoutType = Dropout\n",
    "    else:\n",
    "        raise ValueError('dropoutType must be one of SpatialDropout2D '\n",
    "                         'or Dropout, passed as a string.')\n",
    "\n",
    "    model = Sequential([\n",
    "        # Input(shape = (Chans, Samples, 1)),\n",
    "\n",
    "        # Block 1\n",
    "        Conv2D(F1, (1, kernLength), padding='same',\n",
    "               input_shape=(Chans, Samples, 1),\n",
    "               use_bias=False),\n",
    "        BatchNormalization(),\n",
    "        DepthwiseConv2D((Chans, 1), use_bias=False,\n",
    "                        depth_multiplier=D,\n",
    "                        depthwise_constraint=max_norm(1.)),\n",
    "        BatchNormalization(),\n",
    "        Activation('elu'),\n",
    "        AveragePooling2D((1, 4)),\n",
    "        dropoutType(dropoutRate),\n",
    "\n",
    "        # Block 2\n",
    "        SeparableConv2D(F2, (1, 16), use_bias=False, padding='same'),\n",
    "        BatchNormalization(),\n",
    "        Activation('elu'),\n",
    "        AveragePooling2D((1, 8)),\n",
    "        dropoutType(dropoutRate),\n",
    "\n",
    "        Flatten(name='flatten'),\n",
    "        Dense(nb_classes, name='dense',\n",
    "              kernel_constraint=max_norm(norm_rate)),\n",
    "        Activation('softmax', name='softmax')\n",
    "    ])\n",
    "\n",
    "    optimizer = keras.optimizers.Adam(learning_rate=learning_rate)\n",
    "    model.compile(loss=loss, optimizer=optimizer, metrics=[\"accuracy\"])\n",
    "    return model\n",
    "\n",
    "\n",
    "\n",
    "window_size = 2500\n",
    "channels = 2\n",
    "\n",
    "\n",
    "data, scores, order = get_data();\n",
    "n_participants = len(set(order))\n",
    "data, scores, order = split_data(data, scores, order, window_size=window_size, overlap=100)\n",
    "#data, scores, order = remove_nan(data, scores, order)\n",
    "\n",
    "#scored = scores\n",
    "# one hot encode scores sklearn\n",
    "scores = preprocessing.OneHotEncoder().fit_transform(np.array(scores).reshape(-1, 1))\n",
    "scores = scores.toarray()\n",
    "\n",
    "# use test train split inc order\n",
    "train_data, test_data, train_scores, test_scores, train_order, test_order = train_test_split(data, scores, order,test_size=0.2, random_state=42, shuffle=True)\n",
    "# test data into array\n",
    "test_data = np.array(test_data)\n",
    "#test_scores = test_scores.tolist()\n",
    "history = []\n",
    "\n",
    "# define the checkpoint path and filename\n",
    "checkpoint_path = \"best_model_CNNLSTM.h5\"\n",
    "\n",
    "\n",
    "\n",
    "# leave one out cross validation\n",
    "for i in range(n_participants):\n",
    "    train_datas, train_scoress, val_data, val_scores = remove_participant(train_data, train_scores, train_order, i + 1);\n",
    "    # model\n",
    "    # define model checkpoint callback\n",
    "    checkpoint = ModelCheckpoint(checkpoint_path, monitor='val_accuracy', verbose=1, save_best_only=True, mode='max')\n",
    "\n",
    "    # define early stopping callback\n",
    "    early_stop = EarlyStopping(monitor='val_accuracy', patience=50, mode='max')\n",
    "\n",
    "    models = EEGNet_seq(nb_classes=2, Chans=channels, Samples=window_size, dropoutRate=0.5, kernLength=64, F1=8, D=2, F2=16, norm_rate=0.25, dropoutType='Dropout', learning_rate=3e-3, loss=\"mse\")\n",
    "    hist = models.fit(train_datas, train_scoress, epochs=500, batch_size=512, validation_data=(val_data, val_scores),callbacks=[early_stop, checkpoint])\n",
    "    hist = load_model(checkpoint_path)\n",
    "    history.append(hist);\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31mFailed to connect to the remote Jupyter Server 'http://192.168.50.50:8888/'. Verify the server is running and reachable. (Denied connection to insecure server.)."
     ]
    }
   ],
   "source": [
    "# evaluate all models on test set and save best model\n",
    "best_model = None\n",
    "best_acc = 0\n",
    "# change test_scores to list instead of array\n",
    "# set no logging\n",
    "tf.get_logger().setLevel(3)\n",
    "\n",
    "for i, model in enumerate(history):\n",
    "    # evaluate model on test set\n",
    "    _, acc = model.evaluate(test_data, test_scores, verbose=0)\n",
    "    print('Model %d: %.3f' % (i + 1, acc))\n",
    "    # check if best model\n",
    "    if acc > best_acc:\n",
    "        best_acc = acc\n",
    "        best_model = model\n",
    "\n",
    "# save best model\n",
    "best_model.save(\"best_model_CNNBILSTM_binary.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31mFailed to connect to the remote Jupyter Server 'http://192.168.50.50:8888/'. Verify the server is running and reachable. (Denied connection to insecure server.)."
     ]
    }
   ],
   "source": [
    "# import plot_model, confusion_matrix, plt, sns\n",
    "from tensorflow.keras.utils import plot_model\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# print best model accuracy\n",
    "print('Best Model Accuracy: %.3f' % best_acc)\n",
    "\n",
    "\n",
    "# plot best model\n",
    "plot_model(best_model, to_file='best_model_CNNBILSTM_binary.png', show_shapes=True, show_layer_names=True)\n",
    "# confusion matrix\n",
    "y_pred = best_model.predict(test_data)\n",
    "y_pred = np.argmax(y_pred, axis=1)\n",
    "y_true = np.argmax(test_scores, axis=1)\n",
    "cm = confusion_matrix(y_true, y_pred)\n",
    "# plot confusion matrix\n",
    "plt.figure(figsize=(10, 10))\n",
    "sns.heatmap(cm, annot=True, fmt=\"d\")\n",
    "plt.title(\"Confusion matrix\")\n",
    "plt.ylabel('True label')\n",
    "plt.xlabel('Predicted label')\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31mFailed to connect to the remote Jupyter Server 'http://192.168.50.50:8888/'. Verify the server is running and reachable. (Denied connection to insecure server.)."
     ]
    }
   ],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "vscode": {
   "interpreter": {
    "hash": "50a23ccb2a46e4f3c10b656e01e221a451ca18d9a0bb1860468c4ede0c087d11"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
